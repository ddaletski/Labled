<?xml version="1.0" encoding="utf-8"?>
<network>
	<layers>
		<item>
			<type>Input</type>
			<name>data</name>
			<dst>data</dst>
			<input>
				<shape>
					<item>
						<dim>1 3 224 224</dim>
					</item>
				</shape>
			</input>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>data_bn</name>
			<src>data</src>
			<dst>data_bn</dst>
			<weight>
				<item>
					<dim>3</dim>
				</item>
				<item>
					<dim>3</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>data_scale</name>
			<src>data_bn</src>
			<dst>data_bn</dst>
			<weight>
				<item>
					<dim>3</dim>
				</item>
				<item>
					<dim>3</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>Convolution</type>
			<name>conv1</name>
			<src>data_bn</src>
			<dst>conv1</dst>
			<weight>
				<item>
					<dim>64 3 7 7</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>64</outputNum>
				<kernel>7</kernel>
				<pad>3</pad>
				<stride>2</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>conv1_bn</name>
			<src>conv1</src>
			<dst>conv1</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>conv1_scale</name>
			<src>conv1</src>
			<dst>conv1</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>conv1_relu</name>
			<src>conv1</src>
			<dst>conv1</dst>
		</item>
		<item>
			<type>Pooling</type>
			<name>conv1_pool</name>
			<src>conv1</src>
			<dst>conv1_pool</dst>
			<pooling>
				<method>Max</method>
				<kernel>3</kernel>
				<stride>2</stride>
			</pooling>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_64_1_conv1</name>
			<src>conv1_pool</src>
			<dst>layer_64_1_conv1</dst>
			<weight>
				<item>
					<dim>64 64 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>64</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_64_1_bn2</name>
			<src>layer_64_1_conv1</src>
			<dst>layer_64_1_conv1</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_64_1_scale2</name>
			<src>layer_64_1_conv1</src>
			<dst>layer_64_1_conv1</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_64_1_relu2</name>
			<src>layer_64_1_conv1</src>
			<dst>layer_64_1_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_64_1_conv2</name>
			<src>layer_64_1_conv1</src>
			<dst>layer_64_1_conv2</dst>
			<weight>
				<item>
					<dim>64 64 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>64</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_64_1_bn3</name>
			<src>layer_64_1_conv2</src>
			<dst>layer_64_1_conv2</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_64_1_scale3</name>
			<src>layer_64_1_conv2</src>
			<dst>layer_64_1_conv2</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_64_1_relu3</name>
			<src>layer_64_1_conv2</src>
			<dst>layer_64_1_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_64_1_conv3</name>
			<src>layer_64_1_conv2</src>
			<dst>layer_64_1_conv3</dst>
			<weight>
				<item>
					<dim>256 64 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_64_1_conv_expand</name>
			<src>layer_64_1_conv1</src>
			<dst>layer_64_1_conv_expand</dst>
			<weight>
				<item>
					<dim>256 64 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_64_1_sum</name>
			<src>layer_64_1_conv3 layer_64_1_conv_expand</src>
			<dst>layer_64_1_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_64_2_bn1</name>
			<src>layer_64_1_sum</src>
			<dst>layer_64_2_bn1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_64_2_scale1</name>
			<src>layer_64_2_bn1</src>
			<dst>layer_64_2_bn1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_64_2_relu1</name>
			<src>layer_64_2_bn1</src>
			<dst>layer_64_2_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_64_2_conv1</name>
			<src>layer_64_2_bn1</src>
			<dst>layer_64_2_conv1</dst>
			<weight>
				<item>
					<dim>64 256 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>64</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_64_2_bn2</name>
			<src>layer_64_2_conv1</src>
			<dst>layer_64_2_conv1</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_64_2_scale2</name>
			<src>layer_64_2_conv1</src>
			<dst>layer_64_2_conv1</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_64_2_relu2</name>
			<src>layer_64_2_conv1</src>
			<dst>layer_64_2_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_64_2_conv2</name>
			<src>layer_64_2_conv1</src>
			<dst>layer_64_2_conv2</dst>
			<weight>
				<item>
					<dim>64 64 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>64</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_64_2_bn3</name>
			<src>layer_64_2_conv2</src>
			<dst>layer_64_2_conv2</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_64_2_scale3</name>
			<src>layer_64_2_conv2</src>
			<dst>layer_64_2_conv2</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_64_2_relu3</name>
			<src>layer_64_2_conv2</src>
			<dst>layer_64_2_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_64_2_conv3</name>
			<src>layer_64_2_conv2</src>
			<dst>layer_64_2_conv3</dst>
			<weight>
				<item>
					<dim>256 64 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_64_2_sum</name>
			<src>layer_64_2_conv3 layer_64_1_sum</src>
			<dst>layer_64_2_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_64_3_bn1</name>
			<src>layer_64_2_sum</src>
			<dst>layer_64_3_bn1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_64_3_scale1</name>
			<src>layer_64_3_bn1</src>
			<dst>layer_64_3_bn1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_64_3_relu1</name>
			<src>layer_64_3_bn1</src>
			<dst>layer_64_3_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_64_3_conv1</name>
			<src>layer_64_3_bn1</src>
			<dst>layer_64_3_conv1</dst>
			<weight>
				<item>
					<dim>64 256 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>64</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_64_3_bn2</name>
			<src>layer_64_3_conv1</src>
			<dst>layer_64_3_conv1</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_64_3_scale2</name>
			<src>layer_64_3_conv1</src>
			<dst>layer_64_3_conv1</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_64_3_relu2</name>
			<src>layer_64_3_conv1</src>
			<dst>layer_64_3_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_64_3_conv2</name>
			<src>layer_64_3_conv1</src>
			<dst>layer_64_3_conv2</dst>
			<weight>
				<item>
					<dim>64 64 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>64</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_64_3_bn3</name>
			<src>layer_64_3_conv2</src>
			<dst>layer_64_3_conv2</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_64_3_scale3</name>
			<src>layer_64_3_conv2</src>
			<dst>layer_64_3_conv2</dst>
			<weight>
				<item>
					<dim>64</dim>
				</item>
				<item>
					<dim>64</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_64_3_relu3</name>
			<src>layer_64_3_conv2</src>
			<dst>layer_64_3_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_64_3_conv3</name>
			<src>layer_64_3_conv2</src>
			<dst>layer_64_3_conv3</dst>
			<weight>
				<item>
					<dim>256 64 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_64_3_sum</name>
			<src>layer_64_3_conv3 layer_64_2_sum</src>
			<dst>layer_64_3_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_128_1_bn1</name>
			<src>layer_64_3_sum</src>
			<dst>layer_128_1_bn1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_128_1_scale1</name>
			<src>layer_128_1_bn1</src>
			<dst>layer_128_1_bn1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_128_1_relu1</name>
			<src>layer_128_1_bn1</src>
			<dst>layer_128_1_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_1_conv1</name>
			<src>layer_128_1_bn1</src>
			<dst>layer_128_1_conv1</dst>
			<weight>
				<item>
					<dim>128 256 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>128</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_128_1_bn2</name>
			<src>layer_128_1_conv1</src>
			<dst>layer_128_1_conv1</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_128_1_scale2</name>
			<src>layer_128_1_conv1</src>
			<dst>layer_128_1_conv1</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_128_1_relu2</name>
			<src>layer_128_1_conv1</src>
			<dst>layer_128_1_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_1_conv2</name>
			<src>layer_128_1_conv1</src>
			<dst>layer_128_1_conv2</dst>
			<weight>
				<item>
					<dim>128 128 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>128</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>2</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_128_1_bn3</name>
			<src>layer_128_1_conv2</src>
			<dst>layer_128_1_conv2</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_128_1_scale3</name>
			<src>layer_128_1_conv2</src>
			<dst>layer_128_1_conv2</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_128_1_relu3</name>
			<src>layer_128_1_conv2</src>
			<dst>layer_128_1_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_1_conv3</name>
			<src>layer_128_1_conv2</src>
			<dst>layer_128_1_conv3</dst>
			<weight>
				<item>
					<dim>512 128 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>512</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_1_conv_expand</name>
			<src>layer_128_1_bn1</src>
			<dst>layer_128_1_conv_expand</dst>
			<weight>
				<item>
					<dim>512 256 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>512</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>2</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_128_1_sum</name>
			<src>layer_128_1_conv3 layer_128_1_conv_expand</src>
			<dst>layer_128_1_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_128_2_bn1</name>
			<src>layer_128_1_sum</src>
			<dst>layer_128_2_bn1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_128_2_scale1</name>
			<src>layer_128_2_bn1</src>
			<dst>layer_128_2_bn1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_128_2_relu1</name>
			<src>layer_128_2_bn1</src>
			<dst>layer_128_2_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_2_conv1</name>
			<src>layer_128_2_bn1</src>
			<dst>layer_128_2_conv1</dst>
			<weight>
				<item>
					<dim>128 512 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>128</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_128_2_bn2</name>
			<src>layer_128_2_conv1</src>
			<dst>layer_128_2_conv1</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_128_2_scale2</name>
			<src>layer_128_2_conv1</src>
			<dst>layer_128_2_conv1</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_128_2_relu2</name>
			<src>layer_128_2_conv1</src>
			<dst>layer_128_2_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_2_conv2</name>
			<src>layer_128_2_conv1</src>
			<dst>layer_128_2_conv2</dst>
			<weight>
				<item>
					<dim>128 128 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>128</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_128_2_bn3</name>
			<src>layer_128_2_conv2</src>
			<dst>layer_128_2_conv2</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_128_2_scale3</name>
			<src>layer_128_2_conv2</src>
			<dst>layer_128_2_conv2</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_128_2_relu3</name>
			<src>layer_128_2_conv2</src>
			<dst>layer_128_2_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_2_conv3</name>
			<src>layer_128_2_conv2</src>
			<dst>layer_128_2_conv3</dst>
			<weight>
				<item>
					<dim>512 128 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>512</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_128_2_sum</name>
			<src>layer_128_2_conv3 layer_128_1_sum</src>
			<dst>layer_128_2_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_128_3_bn1</name>
			<src>layer_128_2_sum</src>
			<dst>layer_128_3_bn1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_128_3_scale1</name>
			<src>layer_128_3_bn1</src>
			<dst>layer_128_3_bn1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_128_3_relu1</name>
			<src>layer_128_3_bn1</src>
			<dst>layer_128_3_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_3_conv1</name>
			<src>layer_128_3_bn1</src>
			<dst>layer_128_3_conv1</dst>
			<weight>
				<item>
					<dim>128 512 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>128</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_128_3_bn2</name>
			<src>layer_128_3_conv1</src>
			<dst>layer_128_3_conv1</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_128_3_scale2</name>
			<src>layer_128_3_conv1</src>
			<dst>layer_128_3_conv1</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_128_3_relu2</name>
			<src>layer_128_3_conv1</src>
			<dst>layer_128_3_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_3_conv2</name>
			<src>layer_128_3_conv1</src>
			<dst>layer_128_3_conv2</dst>
			<weight>
				<item>
					<dim>128 128 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>128</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_128_3_bn3</name>
			<src>layer_128_3_conv2</src>
			<dst>layer_128_3_conv2</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_128_3_scale3</name>
			<src>layer_128_3_conv2</src>
			<dst>layer_128_3_conv2</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_128_3_relu3</name>
			<src>layer_128_3_conv2</src>
			<dst>layer_128_3_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_3_conv3</name>
			<src>layer_128_3_conv2</src>
			<dst>layer_128_3_conv3</dst>
			<weight>
				<item>
					<dim>512 128 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>512</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_128_3_sum</name>
			<src>layer_128_3_conv3 layer_128_2_sum</src>
			<dst>layer_128_3_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_128_4_bn1</name>
			<src>layer_128_3_sum</src>
			<dst>layer_128_4_bn1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_128_4_scale1</name>
			<src>layer_128_4_bn1</src>
			<dst>layer_128_4_bn1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_128_4_relu1</name>
			<src>layer_128_4_bn1</src>
			<dst>layer_128_4_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_4_conv1</name>
			<src>layer_128_4_bn1</src>
			<dst>layer_128_4_conv1</dst>
			<weight>
				<item>
					<dim>128 512 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>128</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_128_4_bn2</name>
			<src>layer_128_4_conv1</src>
			<dst>layer_128_4_conv1</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_128_4_scale2</name>
			<src>layer_128_4_conv1</src>
			<dst>layer_128_4_conv1</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_128_4_relu2</name>
			<src>layer_128_4_conv1</src>
			<dst>layer_128_4_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_4_conv2</name>
			<src>layer_128_4_conv1</src>
			<dst>layer_128_4_conv2</dst>
			<weight>
				<item>
					<dim>128 128 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>128</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_128_4_bn3</name>
			<src>layer_128_4_conv2</src>
			<dst>layer_128_4_conv2</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_128_4_scale3</name>
			<src>layer_128_4_conv2</src>
			<dst>layer_128_4_conv2</dst>
			<weight>
				<item>
					<dim>128</dim>
				</item>
				<item>
					<dim>128</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_128_4_relu3</name>
			<src>layer_128_4_conv2</src>
			<dst>layer_128_4_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_128_4_conv3</name>
			<src>layer_128_4_conv2</src>
			<dst>layer_128_4_conv3</dst>
			<weight>
				<item>
					<dim>512 128 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>512</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_128_4_sum</name>
			<src>layer_128_4_conv3 layer_128_3_sum</src>
			<dst>layer_128_4_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_1_bn1</name>
			<src>layer_128_4_sum</src>
			<dst>layer_256_1_bn1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_1_scale1</name>
			<src>layer_256_1_bn1</src>
			<dst>layer_256_1_bn1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_1_relu1</name>
			<src>layer_256_1_bn1</src>
			<dst>layer_256_1_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_1_conv1</name>
			<src>layer_256_1_bn1</src>
			<dst>layer_256_1_conv1</dst>
			<weight>
				<item>
					<dim>256 512 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_1_bn2</name>
			<src>layer_256_1_conv1</src>
			<dst>layer_256_1_conv1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_1_scale2</name>
			<src>layer_256_1_conv1</src>
			<dst>layer_256_1_conv1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_1_relu2</name>
			<src>layer_256_1_conv1</src>
			<dst>layer_256_1_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_1_conv2</name>
			<src>layer_256_1_conv1</src>
			<dst>layer_256_1_conv2</dst>
			<weight>
				<item>
					<dim>256 256 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>2</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_1_bn3</name>
			<src>layer_256_1_conv2</src>
			<dst>layer_256_1_conv2</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_1_scale3</name>
			<src>layer_256_1_conv2</src>
			<dst>layer_256_1_conv2</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_1_relu3</name>
			<src>layer_256_1_conv2</src>
			<dst>layer_256_1_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_1_conv3</name>
			<src>layer_256_1_conv2</src>
			<dst>layer_256_1_conv3</dst>
			<weight>
				<item>
					<dim>1024 256 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>1024</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_1_conv_expand</name>
			<src>layer_256_1_bn1</src>
			<dst>layer_256_1_conv_expand</dst>
			<weight>
				<item>
					<dim>1024 512 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>1024</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>2</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_256_1_sum</name>
			<src>layer_256_1_conv3 layer_256_1_conv_expand</src>
			<dst>layer_256_1_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_2_bn1</name>
			<src>layer_256_1_sum</src>
			<dst>layer_256_2_bn1</dst>
			<weight>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_2_scale1</name>
			<src>layer_256_2_bn1</src>
			<dst>layer_256_2_bn1</dst>
			<weight>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1024</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_2_relu1</name>
			<src>layer_256_2_bn1</src>
			<dst>layer_256_2_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_2_conv1</name>
			<src>layer_256_2_bn1</src>
			<dst>layer_256_2_conv1</dst>
			<weight>
				<item>
					<dim>256 1024 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_2_bn2</name>
			<src>layer_256_2_conv1</src>
			<dst>layer_256_2_conv1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_2_scale2</name>
			<src>layer_256_2_conv1</src>
			<dst>layer_256_2_conv1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_2_relu2</name>
			<src>layer_256_2_conv1</src>
			<dst>layer_256_2_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_2_conv2</name>
			<src>layer_256_2_conv1</src>
			<dst>layer_256_2_conv2</dst>
			<weight>
				<item>
					<dim>256 256 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_2_bn3</name>
			<src>layer_256_2_conv2</src>
			<dst>layer_256_2_conv2</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_2_scale3</name>
			<src>layer_256_2_conv2</src>
			<dst>layer_256_2_conv2</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_2_relu3</name>
			<src>layer_256_2_conv2</src>
			<dst>layer_256_2_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_2_conv3</name>
			<src>layer_256_2_conv2</src>
			<dst>layer_256_2_conv3</dst>
			<weight>
				<item>
					<dim>1024 256 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>1024</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_256_2_sum</name>
			<src>layer_256_2_conv3 layer_256_1_sum</src>
			<dst>layer_256_2_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_3_bn1</name>
			<src>layer_256_2_sum</src>
			<dst>layer_256_3_bn1</dst>
			<weight>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_3_scale1</name>
			<src>layer_256_3_bn1</src>
			<dst>layer_256_3_bn1</dst>
			<weight>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1024</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_3_relu1</name>
			<src>layer_256_3_bn1</src>
			<dst>layer_256_3_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_3_conv1</name>
			<src>layer_256_3_bn1</src>
			<dst>layer_256_3_conv1</dst>
			<weight>
				<item>
					<dim>256 1024 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_3_bn2</name>
			<src>layer_256_3_conv1</src>
			<dst>layer_256_3_conv1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_3_scale2</name>
			<src>layer_256_3_conv1</src>
			<dst>layer_256_3_conv1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_3_relu2</name>
			<src>layer_256_3_conv1</src>
			<dst>layer_256_3_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_3_conv2</name>
			<src>layer_256_3_conv1</src>
			<dst>layer_256_3_conv2</dst>
			<weight>
				<item>
					<dim>256 256 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_3_bn3</name>
			<src>layer_256_3_conv2</src>
			<dst>layer_256_3_conv2</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_3_scale3</name>
			<src>layer_256_3_conv2</src>
			<dst>layer_256_3_conv2</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_3_relu3</name>
			<src>layer_256_3_conv2</src>
			<dst>layer_256_3_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_3_conv3</name>
			<src>layer_256_3_conv2</src>
			<dst>layer_256_3_conv3</dst>
			<weight>
				<item>
					<dim>1024 256 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>1024</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_256_3_sum</name>
			<src>layer_256_3_conv3 layer_256_2_sum</src>
			<dst>layer_256_3_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_4_bn1</name>
			<src>layer_256_3_sum</src>
			<dst>layer_256_4_bn1</dst>
			<weight>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_4_scale1</name>
			<src>layer_256_4_bn1</src>
			<dst>layer_256_4_bn1</dst>
			<weight>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1024</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_4_relu1</name>
			<src>layer_256_4_bn1</src>
			<dst>layer_256_4_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_4_conv1</name>
			<src>layer_256_4_bn1</src>
			<dst>layer_256_4_conv1</dst>
			<weight>
				<item>
					<dim>256 1024 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_4_bn2</name>
			<src>layer_256_4_conv1</src>
			<dst>layer_256_4_conv1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_4_scale2</name>
			<src>layer_256_4_conv1</src>
			<dst>layer_256_4_conv1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_4_relu2</name>
			<src>layer_256_4_conv1</src>
			<dst>layer_256_4_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_4_conv2</name>
			<src>layer_256_4_conv1</src>
			<dst>layer_256_4_conv2</dst>
			<weight>
				<item>
					<dim>256 256 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_4_bn3</name>
			<src>layer_256_4_conv2</src>
			<dst>layer_256_4_conv2</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_4_scale3</name>
			<src>layer_256_4_conv2</src>
			<dst>layer_256_4_conv2</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_4_relu3</name>
			<src>layer_256_4_conv2</src>
			<dst>layer_256_4_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_4_conv3</name>
			<src>layer_256_4_conv2</src>
			<dst>layer_256_4_conv3</dst>
			<weight>
				<item>
					<dim>1024 256 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>1024</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_256_4_sum</name>
			<src>layer_256_4_conv3 layer_256_3_sum</src>
			<dst>layer_256_4_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_5_bn1</name>
			<src>layer_256_4_sum</src>
			<dst>layer_256_5_bn1</dst>
			<weight>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_5_scale1</name>
			<src>layer_256_5_bn1</src>
			<dst>layer_256_5_bn1</dst>
			<weight>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1024</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_5_relu1</name>
			<src>layer_256_5_bn1</src>
			<dst>layer_256_5_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_5_conv1</name>
			<src>layer_256_5_bn1</src>
			<dst>layer_256_5_conv1</dst>
			<weight>
				<item>
					<dim>256 1024 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_5_bn2</name>
			<src>layer_256_5_conv1</src>
			<dst>layer_256_5_conv1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_5_scale2</name>
			<src>layer_256_5_conv1</src>
			<dst>layer_256_5_conv1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_5_relu2</name>
			<src>layer_256_5_conv1</src>
			<dst>layer_256_5_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_5_conv2</name>
			<src>layer_256_5_conv1</src>
			<dst>layer_256_5_conv2</dst>
			<weight>
				<item>
					<dim>256 256 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_5_bn3</name>
			<src>layer_256_5_conv2</src>
			<dst>layer_256_5_conv2</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_5_scale3</name>
			<src>layer_256_5_conv2</src>
			<dst>layer_256_5_conv2</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_5_relu3</name>
			<src>layer_256_5_conv2</src>
			<dst>layer_256_5_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_5_conv3</name>
			<src>layer_256_5_conv2</src>
			<dst>layer_256_5_conv3</dst>
			<weight>
				<item>
					<dim>1024 256 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>1024</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_256_5_sum</name>
			<src>layer_256_5_conv3 layer_256_4_sum</src>
			<dst>layer_256_5_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_6_bn1</name>
			<src>layer_256_5_sum</src>
			<dst>layer_256_6_bn1</dst>
			<weight>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_6_scale1</name>
			<src>layer_256_6_bn1</src>
			<dst>layer_256_6_bn1</dst>
			<weight>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1024</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_6_relu1</name>
			<src>layer_256_6_bn1</src>
			<dst>layer_256_6_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_6_conv1</name>
			<src>layer_256_6_bn1</src>
			<dst>layer_256_6_conv1</dst>
			<weight>
				<item>
					<dim>256 1024 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_6_bn2</name>
			<src>layer_256_6_conv1</src>
			<dst>layer_256_6_conv1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_6_scale2</name>
			<src>layer_256_6_conv1</src>
			<dst>layer_256_6_conv1</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_6_relu2</name>
			<src>layer_256_6_conv1</src>
			<dst>layer_256_6_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_6_conv2</name>
			<src>layer_256_6_conv1</src>
			<dst>layer_256_6_conv2</dst>
			<weight>
				<item>
					<dim>256 256 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>256</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_256_6_bn3</name>
			<src>layer_256_6_conv2</src>
			<dst>layer_256_6_conv2</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_256_6_scale3</name>
			<src>layer_256_6_conv2</src>
			<dst>layer_256_6_conv2</dst>
			<weight>
				<item>
					<dim>256</dim>
				</item>
				<item>
					<dim>256</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_256_6_relu3</name>
			<src>layer_256_6_conv2</src>
			<dst>layer_256_6_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_256_6_conv3</name>
			<src>layer_256_6_conv2</src>
			<dst>layer_256_6_conv3</dst>
			<weight>
				<item>
					<dim>1024 256 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>1024</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_256_6_sum</name>
			<src>layer_256_6_conv3 layer_256_5_sum</src>
			<dst>layer_256_6_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_512_1_bn1</name>
			<src>layer_256_6_sum</src>
			<dst>layer_512_1_bn1</dst>
			<weight>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_512_1_scale1</name>
			<src>layer_512_1_bn1</src>
			<dst>layer_512_1_bn1</dst>
			<weight>
				<item>
					<dim>1024</dim>
				</item>
				<item>
					<dim>1024</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_512_1_relu1</name>
			<src>layer_512_1_bn1</src>
			<dst>layer_512_1_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_512_1_conv1</name>
			<src>layer_512_1_bn1</src>
			<dst>layer_512_1_conv1</dst>
			<weight>
				<item>
					<dim>512 1024 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>512</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_512_1_bn2</name>
			<src>layer_512_1_conv1</src>
			<dst>layer_512_1_conv1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_512_1_scale2</name>
			<src>layer_512_1_conv1</src>
			<dst>layer_512_1_conv1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_512_1_relu2</name>
			<src>layer_512_1_conv1</src>
			<dst>layer_512_1_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_512_1_conv2</name>
			<src>layer_512_1_conv1</src>
			<dst>layer_512_1_conv2</dst>
			<weight>
				<item>
					<dim>512 512 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>512</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>2</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_512_1_bn3</name>
			<src>layer_512_1_conv2</src>
			<dst>layer_512_1_conv2</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_512_1_scale3</name>
			<src>layer_512_1_conv2</src>
			<dst>layer_512_1_conv2</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_512_1_relu3</name>
			<src>layer_512_1_conv2</src>
			<dst>layer_512_1_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_512_1_conv3</name>
			<src>layer_512_1_conv2</src>
			<dst>layer_512_1_conv3</dst>
			<weight>
				<item>
					<dim>2048 512 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>2048</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_512_1_conv_expand</name>
			<src>layer_512_1_bn1</src>
			<dst>layer_512_1_conv_expand</dst>
			<weight>
				<item>
					<dim>2048 1024 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>2048</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>2</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_512_1_sum</name>
			<src>layer_512_1_conv3 layer_512_1_conv_expand</src>
			<dst>layer_512_1_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_512_2_bn1</name>
			<src>layer_512_1_sum</src>
			<dst>layer_512_2_bn1</dst>
			<weight>
				<item>
					<dim>2048</dim>
				</item>
				<item>
					<dim>2048</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_512_2_scale1</name>
			<src>layer_512_2_bn1</src>
			<dst>layer_512_2_bn1</dst>
			<weight>
				<item>
					<dim>2048</dim>
				</item>
				<item>
					<dim>2048</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_512_2_relu1</name>
			<src>layer_512_2_bn1</src>
			<dst>layer_512_2_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_512_2_conv1</name>
			<src>layer_512_2_bn1</src>
			<dst>layer_512_2_conv1</dst>
			<weight>
				<item>
					<dim>512 2048 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>512</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_512_2_bn2</name>
			<src>layer_512_2_conv1</src>
			<dst>layer_512_2_conv1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_512_2_scale2</name>
			<src>layer_512_2_conv1</src>
			<dst>layer_512_2_conv1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_512_2_relu2</name>
			<src>layer_512_2_conv1</src>
			<dst>layer_512_2_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_512_2_conv2</name>
			<src>layer_512_2_conv1</src>
			<dst>layer_512_2_conv2</dst>
			<weight>
				<item>
					<dim>512 512 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>512</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_512_2_bn3</name>
			<src>layer_512_2_conv2</src>
			<dst>layer_512_2_conv2</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_512_2_scale3</name>
			<src>layer_512_2_conv2</src>
			<dst>layer_512_2_conv2</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_512_2_relu3</name>
			<src>layer_512_2_conv2</src>
			<dst>layer_512_2_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_512_2_conv3</name>
			<src>layer_512_2_conv2</src>
			<dst>layer_512_2_conv3</dst>
			<weight>
				<item>
					<dim>2048 512 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>2048</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_512_2_sum</name>
			<src>layer_512_2_conv3 layer_512_1_sum</src>
			<dst>layer_512_2_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_512_3_bn1</name>
			<src>layer_512_2_sum</src>
			<dst>layer_512_3_bn1</dst>
			<weight>
				<item>
					<dim>2048</dim>
				</item>
				<item>
					<dim>2048</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_512_3_scale1</name>
			<src>layer_512_3_bn1</src>
			<dst>layer_512_3_bn1</dst>
			<weight>
				<item>
					<dim>2048</dim>
				</item>
				<item>
					<dim>2048</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_512_3_relu1</name>
			<src>layer_512_3_bn1</src>
			<dst>layer_512_3_bn1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_512_3_conv1</name>
			<src>layer_512_3_bn1</src>
			<dst>layer_512_3_conv1</dst>
			<weight>
				<item>
					<dim>512 2048 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>512</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_512_3_bn2</name>
			<src>layer_512_3_conv1</src>
			<dst>layer_512_3_conv1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_512_3_scale2</name>
			<src>layer_512_3_conv1</src>
			<dst>layer_512_3_conv1</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_512_3_relu2</name>
			<src>layer_512_3_conv1</src>
			<dst>layer_512_3_conv1</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_512_3_conv2</name>
			<src>layer_512_3_conv1</src>
			<dst>layer_512_3_conv2</dst>
			<weight>
				<item>
					<dim>512 512 3 3</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>512</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>3</kernel>
				<pad>1</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>layer_512_3_bn3</name>
			<src>layer_512_3_conv2</src>
			<dst>layer_512_3_conv2</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>layer_512_3_scale3</name>
			<src>layer_512_3_conv2</src>
			<dst>layer_512_3_conv2</dst>
			<weight>
				<item>
					<dim>512</dim>
				</item>
				<item>
					<dim>512</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>layer_512_3_relu3</name>
			<src>layer_512_3_conv2</src>
			<dst>layer_512_3_conv2</dst>
		</item>
		<item>
			<type>Convolution</type>
			<name>layer_512_3_conv3</name>
			<src>layer_512_3_conv2</src>
			<dst>layer_512_3_conv3</dst>
			<weight>
				<item>
					<dim>2048 512 1 1</dim>
				</item>
			</weight>
			<convolution>
				<outputNum>2048</outputNum>
				<biasTerm>0</biasTerm>
				<kernel>1</kernel>
				<pad>0</pad>
				<stride>1</stride>
			</convolution>
		</item>
		<item>
			<type>Eltwise</type>
			<name>layer_512_3_sum</name>
			<src>layer_512_3_conv3 layer_512_2_sum</src>
			<dst>layer_512_3_sum</dst>
		</item>
		<item>
			<type>BatchNorm</type>
			<name>last_bn</name>
			<src>layer_512_3_sum</src>
			<dst>layer_512_3_sum</dst>
			<weight>
				<item>
					<dim>2048</dim>
				</item>
				<item>
					<dim>2048</dim>
				</item>
				<item>
					<dim>1</dim>
				</item>
			</weight>
		</item>
		<item>
			<type>Scale</type>
			<name>last_scale</name>
			<src>layer_512_3_sum</src>
			<dst>layer_512_3_sum</dst>
			<weight>
				<item>
					<dim>2048</dim>
				</item>
				<item>
					<dim>2048</dim>
				</item>
			</weight>
			<scale>
				<biasTerm>1</biasTerm>
			</scale>
		</item>
		<item>
			<type>ReLU</type>
			<name>last_relu</name>
			<src>layer_512_3_sum</src>
			<dst>layer_512_3_sum</dst>
		</item>
		<item>
			<type>Pooling</type>
			<name>global_pool</name>
			<src>layer_512_3_sum</src>
			<dst>global_pool</dst>
			<pooling>
				<method>Average</method>
				<globalPooling>1</globalPooling>
			</pooling>
		</item>
		<item>
			<type>InnerProduct</type>
			<name>score</name>
			<src>global_pool</src>
			<dst>score</dst>
			<weight>
				<item>
					<dim>1000 2048</dim>
				</item>
				<item>
					<dim>1000</dim>
				</item>
			</weight>
			<innerProduct>
				<outputNum>1000</outputNum>
			</innerProduct>
		</item>
		<item>
			<type>Softmax</type>
			<name>prob</name>
			<src>score</src>
			<dst>prob</dst>
		</item>
	</layers>
</network>

